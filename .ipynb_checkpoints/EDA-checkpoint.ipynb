{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토스 CTR 예측 EDA 🚀 (최종 안정화 버전)\n",
    "\n",
    "**목표:** 단일 GPU 환경에서 발생하는 모든 메모리 오류를 근본적으로 해결하고, 대용량 Parquet 파일을 안정적으로 로드하여 분석합니다.\n",
    "\n",
    "**최종 해결 전략**:\n",
    "- **PyArrow를 이용한 CPU 기반 분할 로드**: `cudf`가 직접 파일을 읽을 때 발생하는 메모리 문제를 우회하기 위해, CPU 기반 라이브러리인 `pyarrow`를 사용합니다.\n",
    "- **점진적 GPU 전송**: `pyarrow`로 파일을 매우 작은 행(row) 단위로 순차적으로 읽어 CPU RAM에 올린 뒤, 이 작은 조각들을 하나씩 GPU 메모리로 안전하게 전송합니다.\n",
    "- **즉시 최적화**: 각 조각이 GPU로 전송된 직후 메모리 최적화를 수행하여 VRAM 사용량을 최소화합니다.\n",
    "- **순수 cuDF로 분석**: 모든 조각이 GPU에 안전하게 로드되고 하나로 합쳐진 이후에는, 빠르고 직관적인 표준 `cudf`로 모든 EDA를 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 준비 및 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# 시각화 라이브러리\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "print(f\"cuDF version: {cudf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_memory(df):\n",
    "    \"\"\" 데이터프레임의 메모리 사용량을 최적화하는 함수 \"\"\"\n",
    "    int_cols = df.select_dtypes(include=['int64', 'int32']).columns\n",
    "    for col in int_cols:\n",
    "        max_val = df[col].max()\n",
    "        min_val = df[col].min()\n",
    "        if max_val < 127 and min_val > -128:\n",
    "            df[col] = df[col].astype('int8')\n",
    "        elif max_val < 32767 and min_val > -32768:\n",
    "            df[col] = df[col].astype('int16')\n",
    "        elif max_val < 2147483647 and min_val > -2147483648:\n",
    "            df[col] = df[col].astype('int32')\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].astype('float32')\n",
    "    return df\n",
    "\n",
    "# --- PyArrow를 이용한 데이터 분할 로드 실행 ---\n",
    "TRAIN_PATH = 'data/train.parquet'\n",
    "\n",
    "# 1. PyArrow를 사용하여 CPU에서 Parquet 파일을 엽니다.\n",
    "parquet_file = pq.ParquetFile(TRAIN_PATH)\n",
    "batch_size = 1_000_000  # 한 번에 읽을 행의 수 (CPU RAM 및 GPU VRAM에 따라 조절)\n",
    "print(f\"파일을 {batch_size:,} 행 단위의 작은 조각으로 나누어 읽습니다.\")\n",
    "\n",
    "# 2. 파일의 각 조각을 순회하며 읽고, GPU로 옮긴 후 최적화합니다.\n",
    "optimized_chunks = []\n",
    "for i, batch in enumerate(parquet_file.iter_batches(batch_size=batch_size)):\n",
    "    print(f\"  - {i+1}번째 조각 처리 중...\")\n",
    "    # PyArrow Batch -> Pandas DataFrame으로 변환 (CPU)\n",
    "    pdf = batch.to_pandas()\n",
    "    # Pandas DataFrame -> cuDF DataFrame으로 변환 (CPU -> GPU 전송)\n",
    "    temp_df = cudf.from_pandas(pdf)\n",
    "    \n",
    "    # 메모리 최적화 바로 적용\n",
    "    optimized_df = optimize_memory(temp_df)\n",
    "    optimized_chunks.append(optimized_df)\n",
    "    \n",
    "    # 메모리 확보\n",
    "    del pdf, temp_df, optimized_df\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "# 3. 최적화된 모든 조각들을 하나로 합치기\n",
    "print(\"\\n최적화된 조각들을 하나로 병합합니다...\")\n",
    "train_df = cudf.concat(optimized_chunks, ignore_index=True)\n",
    "del optimized_chunks\n",
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "print(\"\\n데이터 로드 및 최적화 완료! ✨\")\n",
    "print(f\"Final Optimized Memory Usage: {train_df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 기본 정보 확인 (Basic Information)\n",
    "이제부터는 모든 작업을 `train_df` 라는 일반 `cudf` 데이터프레임으로 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train 데이터 형태: {train_df.shape}\")\n",
    "print(f\"샘플 수: {train_df.shape[0]:,} 개\")\n",
    "print(f\"컬럼 수: {train_df.shape[1]} 개\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 타겟 변수(`clicked`) 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_counts = train_df['clicked'].value_counts().to_pandas()\n",
    "total_ctr = (train_df['clicked'].mean() * 100).item()\n",
    "\n",
    "fig = px.bar(clicked_counts, \n",
    "             x=clicked_counts.index, \n",
    "             y=clicked_counts.values, \n",
    "             labels={'x': 'Clicked', 'y': 'Count'},\n",
    "             title=f'클릭 여부 분포 (전체 CTR: {total_ctr:.4f}%)',\n",
    "             text_auto=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 주요 피처 분석 (Key Feature Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_features = ['gender', 'age_group', 'day_of_week', 'hour']\n",
    "\n",
    "def plot_ctr_by_feature(df, feature):\n",
    "    grouped = df.groupby(feature)['clicked'].agg(['count', 'sum']).to_pandas().reset_index()\n",
    "    grouped['ctr'] = (grouped['sum'] / grouped['count']) * 100\n",
    "    grouped = grouped.sort_values('ctr', ascending=False)\n",
    "    \n",
    "    fig = px.bar(grouped, \n",
    "                 x=feature, \n",
    "                 y='ctr', \n",
    "                 color='count',\n",
    "                 color_continuous_scale=px.colors.sequential.Viridis,\n",
    "                 title=f'{feature}에 따른 CTR',\n",
    "                 labels={'ctr': 'CTR (%)', 'count': '노출 수'},\n",
    "                 text_auto='.4f')\n",
    "    fig.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "    fig.show()\n",
    "\n",
    "for feature in main_features:\n",
    "    plot_ctr_by_feature(train_df, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 익명화 피처 탐색 (Anonymous Feature Exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_high_cardinality_feature(df, feature):\n",
    "    print(f\"--- {feature} 피처 분석 ---\")\n",
    "    num_unique = df[feature].nunique()\n",
    "    print(f\"고유 값 개수: {num_unique}\")\n",
    "    \n",
    "    grouped = df.groupby(feature)['clicked'].agg(['count', 'sum']).to_pandas().reset_index()\n",
    "    grouped['ctr'] = (grouped['sum'] / grouped['count']) * 100\n",
    "    \n",
    "    # 노출 수 기준 상위 10개\n",
    "    top_10_by_count = grouped.sort_values('count', ascending=False).head(10)\n",
    "    fig1 = px.bar(top_10_by_count, x=feature, y='count', title=f'{feature}별 노출 수 TOP 10', text_auto=True)\n",
    "    fig1.update_layout(xaxis_type='category')\n",
    "    fig1.show()\n",
    "\n",
    "    # CTR 기준 상위 10개 (단, 노출이 최소 1000번 이상인 경우만)\n",
    "    top_10_by_ctr = grouped[grouped['count'] >= 1000].sort_values('ctr', ascending=False).head(10)\n",
    "    fig2 = px.bar(top_10_by_ctr, x=feature, y='ctr', title=f'{feature}별 CTR TOP 10', text_auto='.4f')\n",
    "    fig2.update_layout(xaxis_type='category')\n",
    "    fig2.show()\n",
    "\n",
    "analyze_high_cardinality_feature(train_df, 'inventory_id')\n",
    "analyze_high_cardinality_feature(train_df, 'l_feat_14')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemail": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
