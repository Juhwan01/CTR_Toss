{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA Merlin XGBoost\n",
    "Complete implementation with proper memory management and debugging outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment configured\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… nvtabular       23.08.00        (required: â‰¥23.08.00)\n",
      "âœ… cudf            23.10.00        (required: â‰¥23.10)\n",
      "âœ… cupy            13.6.0          (required: â‰¥13.6)\n",
      "âœ… xgboost         3.0.5           (required: â‰¥3.0)\n",
      "âœ… dask            2023.9.2        (required: â‰¥2023.9)\n",
      "âœ… pandas          1.5.3           (required: â‰¥1.5)\n",
      "âœ… numpy           1.23.5          (required: â‰¥1.24)\n",
      "âœ… scikit-learn    1.7.2           (required: â‰¥1.7)\n",
      "âœ… psutil          5.9.8           (required: â‰¥5.9)\n",
      "âœ… pyarrow         12.0.1          (required: â‰¥12.0)\n",
      "\n",
      "âœ… All required libraries are installed and compatible!\n"
     ]
    }
   ],
   "source": [
    "# Required libraries and versions\n",
    "required_libs = {\n",
    "    'nvtabular': '23.08.00',\n",
    "    'cudf': '23.10',      # Prefix match\n",
    "    'cupy': '13.6',       # Prefix match  \n",
    "    'xgboost': '3.0',     # Minimum version\n",
    "    'dask': '2023.9',\n",
    "    'pandas': '1.5',\n",
    "    'numpy': '1.24',\n",
    "    'scikit-learn': '1.7',\n",
    "    'psutil': '5.9',      # 5.9.1 works fine (used in working code)\n",
    "    'pyarrow': '12.0'     # 12.0.1 works fine (used in working code)\n",
    "}\n",
    "\n",
    "# Check installed versions\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "# Suppress deprecation warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    try:\n",
    "        import pkg_resources\n",
    "    except:\n",
    "        pkg_resources = None\n",
    "\n",
    "missing_libs = []\n",
    "all_good = True\n",
    "\n",
    "for lib, required_version in required_libs.items():\n",
    "    try:\n",
    "        # Map library names for import\n",
    "        import_name = lib\n",
    "        if lib == 'scikit-learn':\n",
    "            import_name = 'sklearn'\n",
    "        \n",
    "        # Check if library is installed\n",
    "        module = importlib.import_module(import_name)\n",
    "        \n",
    "        # Get installed version\n",
    "        try:\n",
    "            if hasattr(module, '__version__'):\n",
    "                installed_version = module.__version__\n",
    "            elif pkg_resources:\n",
    "                installed_version = pkg_resources.get_distribution(lib).version\n",
    "            else:\n",
    "                installed_version = 'unknown'\n",
    "        except:\n",
    "            installed_version = 'unknown'\n",
    "        \n",
    "        # Check version compatibility\n",
    "        req_major = required_version.split('.')[0]\n",
    "        inst_version_parts = installed_version.split('.')\n",
    "        inst_major = inst_version_parts[0] if installed_version != 'unknown' else ''\n",
    "        \n",
    "        # More lenient version check\n",
    "        if installed_version == 'unknown':\n",
    "            print(f\"âš ï¸  {lib:15} {installed_version:15} (required: â‰¥{required_version})\")\n",
    "        elif float(inst_major) >= float(req_major) if inst_major.isdigit() and req_major.isdigit() else installed_version.startswith(required_version[:3]):\n",
    "            print(f\"âœ… {lib:15} {installed_version:15} (required: â‰¥{required_version})\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  {lib:15} {installed_version:15} (required: â‰¥{required_version}) - but should work\")\n",
    "        \n",
    "    except ImportError:\n",
    "        missing_libs.append(lib)\n",
    "        print(f\"âŒ {lib:15} NOT INSTALLED (required: â‰¥{required_version})\")\n",
    "        all_good = False\n",
    "\n",
    "# Report\n",
    "if missing_libs:\n",
    "    print(f\"\\nâŒ Missing libraries: {', '.join(missing_libs)}\")\n",
    "    print(\"Please install them using conda or pip\")\n",
    "elif all_good:\n",
    "    print(\"\\nâœ… All required libraries are installed and compatible!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully\n",
      "NVTabular version: 23.08.00\n",
      "XGBoost version: 3.0.5\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import psutil\n",
    "\n",
    "# GPU libraries\n",
    "import cudf\n",
    "import cupy as cp\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "from nvtabular import ops\n",
    "from merlin.io import Dataset\n",
    "\n",
    "# ML libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")\n",
    "print(f\"NVTabular version: {nvt.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Configuration:\n",
      "   Input: data/train.parquet\n",
      "   Output: data/nvt_processed_final\n",
      "   Folds: 5\n",
      "   Force reprocess: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration(DATA PATH)\n",
    "TRAIN_PATH = 'data/train.parquet'\n",
    "OUTPUT_DIR = 'data/nvt_processed_final'\n",
    "TEMP_DIR = '/tmp'\n",
    "N_FOLDS = 5\n",
    "FORCE_REPROCESS = False  # Set to True to reprocess data\n",
    "\n",
    "print(f\"ğŸ“‹ Configuration:\")\n",
    "print(f\"   Input: {TRAIN_PATH}\")\n",
    "print(f\"   Output: {OUTPUT_DIR}\")\n",
    "print(f\"   Folds: {N_FOLDS}\")\n",
    "print(f\"   Force reprocess: {FORCE_REPROCESS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing memory functions:\n",
      "ğŸ’¾ CPU: 27.6GB/503.5GB (6.2%)\n",
      "ğŸ’¾ GPU: 0.8GB/45.0GB\n",
      "ğŸ§¹ GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Memory management functions\n",
    "def print_memory():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    \n",
    "    try:\n",
    "        import pynvml\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_used = gpu_info.used / 1024**3\n",
    "        gpu_total = gpu_info.total / 1024**3\n",
    "    except:\n",
    "        gpu_used = 0\n",
    "        gpu_total = 0\n",
    "    \n",
    "    print(f\"ğŸ’¾ CPU: {mem.used/1024**3:.1f}GB/{mem.total/1024**3:.1f}GB ({mem.percent:.1f}%)\")\n",
    "    print(f\"ğŸ’¾ GPU: {gpu_used:.1f}GB/{gpu_total:.1f}GB\")\n",
    "    return mem.percent\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    gc.collect()\n",
    "    print(\"ğŸ§¹ GPU memory cleared\")\n",
    "\n",
    "# Test memory functions\n",
    "print(\"Testing memory functions:\")\n",
    "print_memory()\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Metric functions defined\n"
     ]
    }
   ],
   "source": [
    "# Metric functions\n",
    "def calculate_weighted_logloss(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"Calculate Weighted LogLoss with 50:50 class weights\"\"\"\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    \n",
    "    mask_0 = (y_true == 0)\n",
    "    mask_1 = (y_true == 1)\n",
    "    \n",
    "    ll_0 = -np.mean(np.log(1 - y_pred[mask_0])) if mask_0.sum() > 0 else 0\n",
    "    ll_1 = -np.mean(np.log(y_pred[mask_1])) if mask_1.sum() > 0 else 0\n",
    "    \n",
    "    return 0.5 * ll_0 + 0.5 * ll_1\n",
    "\n",
    "def calculate_competition_score(y_true, y_pred):\n",
    "    \"\"\"Calculate competition score: 0.5*AP + 0.5*(1/(1+WLL))\"\"\"\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    wll = calculate_weighted_logloss(y_true, y_pred)\n",
    "    score = 0.5 * ap + 0.5 * (1 / (1 + wll))\n",
    "    return score, ap, wll\n",
    "\n",
    "print(\"âœ… Metric functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”§ Creating XGBoost-optimized workflow...\n",
      "   Categorical: 5 columns\n",
      "   Continuous: 112 columns\n",
      "   Total features: 117\n",
      "   âœ… Workflow created (no normalization for tree models)\n",
      "âœ… Workflow creation tested successfully\n"
     ]
    }
   ],
   "source": [
    "def create_workflow():\n",
    "    \"\"\"Create NVTabular workflow optimized for XGBoost\"\"\"\n",
    "    print(\"\\nğŸ”§ Creating XGBoost-optimized workflow...\")\n",
    "    \n",
    "    # TRUE CATEGORICAL COLUMNS (only 5)\n",
    "    true_categorical = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "    \n",
    "    # CONTINUOUS COLUMNS (112 total)\n",
    "    all_continuous = (\n",
    "        [f'feat_a_{i}' for i in range(1, 19)] +  # 18\n",
    "        [f'feat_b_{i}' for i in range(1, 7)] +   # 6\n",
    "        [f'feat_c_{i}' for i in range(1, 9)] +   # 8\n",
    "        [f'feat_d_{i}' for i in range(1, 7)] +   # 6\n",
    "        [f'feat_e_{i}' for i in range(1, 11)] +  # 10\n",
    "        [f'history_a_{i}' for i in range(1, 8)] +  # 7\n",
    "        [f'history_b_{i}' for i in range(1, 31)] + # 30\n",
    "        [f'l_feat_{i}' for i in range(1, 28)]      # 27\n",
    "    )\n",
    "    \n",
    "    print(f\"   Categorical: {len(true_categorical)} columns\")\n",
    "    print(f\"   Continuous: {len(all_continuous)} columns\")\n",
    "    print(f\"   Total features: {len(true_categorical) + len(all_continuous)}\")\n",
    "    \n",
    "    # Minimal preprocessing for XGBoost\n",
    "    cat_features = true_categorical >> ops.Categorify(\n",
    "        freq_threshold=0,\n",
    "        max_size=50000\n",
    "    )\n",
    "    cont_features = all_continuous >> ops.FillMissing(fill_val=0)\n",
    "    \n",
    "    workflow = nvt.Workflow(cat_features + cont_features + ['clicked'])\n",
    "    \n",
    "    print(\"   âœ… Workflow created (no normalization for tree models)\")\n",
    "    return workflow\n",
    "\n",
    "# Test workflow creation\n",
    "test_workflow = create_workflow()\n",
    "print(\"âœ… Workflow creation tested successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸš€ NVTabular Data Processing\n",
      "======================================================================\n",
      "ğŸ’¾ CPU: 27.6GB/503.5GB (6.2%)\n",
      "ğŸ’¾ GPU: 0.8GB/45.0GB\n",
      "\n",
      "ğŸ“‹ Creating temp file without 'seq' column...\n",
      "   Total columns: 119\n",
      "   Using columns: 118 (excluded 'seq')\n",
      "   Loaded 10,704,179 rows\n",
      "   âœ… Temp file created\n",
      "\n",
      "ğŸ“¦ Creating NVTabular Dataset...\n",
      "   Using 32MB partitions for memory efficiency\n",
      "ğŸ§¹ GPU memory cleared\n",
      "   âœ… Dataset created\n",
      "\n",
      "ğŸ“Š Fitting workflow...\n",
      "\n",
      "ğŸ”§ Creating XGBoost-optimized workflow...\n",
      "   Categorical: 5 columns\n",
      "   Continuous: 112 columns\n",
      "   Total features: 117\n",
      "   âœ… Workflow created (no normalization for tree models)\n",
      "   âœ… Workflow fitted\n",
      "\n",
      "ğŸ’¾ Transforming and saving to data/nvt_processed_final...\n",
      "ğŸ§¹ GPU memory cleared\n",
      "   âœ… Data processed and saved\n",
      "   âœ… Workflow saved to data/nvt_processed_final/workflow\n",
      "ğŸ’¾ CPU: 32.7GB/503.5GB (7.2%)\n",
      "ğŸ’¾ GPU: 26.0GB/45.0GB\n",
      "\n",
      "âœ… Processing complete!\n",
      "   Time: 70.6s\n",
      "   Memory increase: +1.0%\n",
      "ğŸ§¹ GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "def process_data():\n",
    "    \"\"\"Process data with NVTabular\"\"\"\n",
    "    import shutil\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸš€ NVTabular Data Processing\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check if already processed\n",
    "    if os.path.exists(OUTPUT_DIR) and not FORCE_REPROCESS:\n",
    "        try:\n",
    "            test_dataset = Dataset(OUTPUT_DIR, engine='parquet')\n",
    "            print(f\"âœ… Using existing processed data from {OUTPUT_DIR}\")\n",
    "            return OUTPUT_DIR\n",
    "        except:\n",
    "            print(f\"âš ï¸ Existing data corrupted, reprocessing...\")\n",
    "            shutil.rmtree(OUTPUT_DIR)\n",
    "    \n",
    "    # Clear existing if needed\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        print(f\"ğŸ—‘ï¸ Removing existing directory {OUTPUT_DIR}\")\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    initial_mem = print_memory()\n",
    "    \n",
    "    # Prepare data without 'seq' column\n",
    "    temp_path = f'{TEMP_DIR}/train_no_seq.parquet'\n",
    "    if not os.path.exists(temp_path):\n",
    "        print(\"\\nğŸ“‹ Creating temp file without 'seq' column...\")\n",
    "        pf = pq.ParquetFile(TRAIN_PATH)\n",
    "        cols = [c for c in pf.schema.names if c != 'seq']\n",
    "        print(f\"   Total columns: {len(pf.schema.names)}\")\n",
    "        print(f\"   Using columns: {len(cols)} (excluded 'seq')\")\n",
    "        \n",
    "        df = pd.read_parquet(TRAIN_PATH, columns=cols)\n",
    "        print(f\"   Loaded {len(df):,} rows\")\n",
    "        df.to_parquet(temp_path, index=False)\n",
    "        del df\n",
    "        gc.collect()\n",
    "        print(\"   âœ… Temp file created\")\n",
    "    else:\n",
    "        print(f\"âœ… Using existing temp file: {temp_path}\")\n",
    "    \n",
    "    # Create dataset with small partitions\n",
    "    print(\"\\nğŸ“¦ Creating NVTabular Dataset...\")\n",
    "    print(\"   Using 32MB partitions for memory efficiency\")\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    dataset = Dataset(\n",
    "        temp_path,\n",
    "        engine='parquet',\n",
    "        part_size='32MB'  #change size based on your environment\n",
    "    )\n",
    "    print(\"   âœ… Dataset created\")\n",
    "    \n",
    "    # Create and fit workflow\n",
    "    print(\"\\nğŸ“Š Fitting workflow...\")\n",
    "    workflow = create_workflow()\n",
    "    workflow.fit(dataset)\n",
    "    print(\"   âœ… Workflow fitted\")\n",
    "    \n",
    "    # Transform and save\n",
    "    print(f\"\\nğŸ’¾ Transforming and saving to {OUTPUT_DIR}...\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    try:\n",
    "        workflow.transform(dataset).to_parquet(\n",
    "            output_path=OUTPUT_DIR,\n",
    "            shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "            out_files_per_proc=8\n",
    "        )\n",
    "        \n",
    "        workflow_path = f'{OUTPUT_DIR}/workflow'\n",
    "        workflow.save(workflow_path)\n",
    "        print(f\"   âœ… Data processed and saved\")\n",
    "        print(f\"   âœ… Workflow saved to {workflow_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during processing: {e}\")\n",
    "        if os.path.exists(OUTPUT_DIR):\n",
    "            shutil.rmtree(OUTPUT_DIR)\n",
    "        raise\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    final_mem = print_memory()\n",
    "    \n",
    "    print(f\"\\nâœ… Processing complete!\")\n",
    "    print(f\"   Time: {elapsed:.1f}s\")\n",
    "    print(f\"   Memory increase: +{final_mem - initial_mem:.1f}%\")\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    return OUTPUT_DIR\n",
    "\n",
    "# Process data\n",
    "processed_dir = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ”„ Stratified KFold Cross-Validation\n",
      "======================================================================\n",
      "\n",
      "ğŸ“¦ Loading processed data...\n",
      "   Converting to GPU DataFrame...\n",
      "   âœ… Loaded 10,704,179 rows x 118 columns\n",
      "   Time: 14.2s\n",
      "ğŸ’¾ CPU: 37.5GB/503.5GB (8.2%)\n",
      "ğŸ’¾ GPU: 5.9GB/45.0GB\n",
      "\n",
      "ğŸ“Š Preparing data for XGBoost...\n",
      "   Shape: (10704179, 117)\n",
      "   Features: 117\n",
      "   Samples: 10,704,179\n",
      "\n",
      "ğŸ“Š Class distribution:\n",
      "   Positive ratio: 0.0191\n",
      "   Scale pos weight: 51.43\n",
      "ğŸ§¹ GPU memory cleared\n",
      "\n",
      "ğŸ”„ Starting cross-validation...\n",
      "\n",
      "ğŸ“ Fold 1/5\n",
      "   Train: 8,563,343 | Val: 2,140,836\n",
      "   Training...\n",
      "   ğŸ“Š Results:\n",
      "      Score: 0.352895\n",
      "      AP: 0.080719\n",
      "      WLL: 0.599819\n",
      "      Best iteration: 199\n",
      "   â±ï¸ Time: 32.4s\n",
      "ğŸ§¹ GPU memory cleared\n",
      "\n",
      "ğŸ“ Fold 2/5\n",
      "   Train: 8,563,343 | Val: 2,140,836\n",
      "   Training...\n",
      "   ğŸ“Š Results:\n",
      "      Score: 0.353203\n",
      "      AP: 0.081334\n",
      "      WLL: 0.599817\n",
      "      Best iteration: 199\n",
      "   â±ï¸ Time: 30.6s\n",
      "ğŸ§¹ GPU memory cleared\n",
      "\n",
      "ğŸ“ Fold 3/5\n",
      "   Train: 8,563,343 | Val: 2,140,836\n",
      "   Training...\n",
      "   ğŸ“Š Results:\n",
      "      Score: 0.352804\n",
      "      AP: 0.080146\n",
      "      WLL: 0.598821\n",
      "      Best iteration: 199\n",
      "   â±ï¸ Time: 30.6s\n",
      "ğŸ§¹ GPU memory cleared\n",
      "\n",
      "ğŸ“ Fold 4/5\n",
      "   Train: 8,563,343 | Val: 2,140,836\n",
      "   Training...\n",
      "   ğŸ“Š Results:\n",
      "      Score: 0.353666\n",
      "      AP: 0.081622\n",
      "      WLL: 0.598184\n",
      "      Best iteration: 199\n",
      "   â±ï¸ Time: 29.4s\n",
      "ğŸ§¹ GPU memory cleared\n",
      "\n",
      "ğŸ“ Fold 5/5\n",
      "   Train: 8,563,344 | Val: 2,140,835\n",
      "   Training...\n",
      "   ğŸ“Š Results:\n",
      "      Score: 0.353254\n",
      "      AP: 0.081446\n",
      "      WLL: 0.599839\n",
      "      Best iteration: 199\n",
      "   â±ï¸ Time: 29.6s\n",
      "ğŸ§¹ GPU memory cleared\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š Final Cross-Validation Results\n",
      "======================================================================\n",
      "\n",
      "ğŸ† Competition Score: 0.353164 Â± 0.000305\n",
      "ğŸ“ˆ Average Precision: 0.081053 Â± 0.000546\n",
      "ğŸ“‰ Weighted LogLoss: 0.599296 Â± 0.000678\n",
      "\n",
      "All fold scores: ['0.352895', '0.353203', '0.352804', '0.353666', '0.353254']\n"
     ]
    }
   ],
   "source": [
    "def run_cv(processed_dir, n_folds=5):\n",
    "    \"\"\"Run stratified cross-validation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ”„ Stratified KFold Cross-Validation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load processed data\n",
    "    print(\"\\nğŸ“¦ Loading processed data...\")\n",
    "    start_load = time.time()\n",
    "    \n",
    "    try:\n",
    "        dataset = Dataset(processed_dir, engine='parquet', part_size='256MB')\n",
    "        print(\"   Converting to GPU DataFrame...\")\n",
    "        gdf = dataset.to_ddf().compute()\n",
    "        print(f\"   âœ… Loaded {len(gdf):,} rows x {len(gdf.columns)} columns\")\n",
    "        print(f\"   Time: {time.time() - start_load:.1f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print_memory()\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\nğŸ“Š Preparing data for XGBoost...\")\n",
    "    y = gdf['clicked'].to_numpy()\n",
    "    X = gdf.drop('clicked', axis=1)\n",
    "    \n",
    "    # Convert to float32\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype != 'float32':\n",
    "            X[col] = X[col].astype('float32')\n",
    "    \n",
    "    X_np = X.to_numpy()\n",
    "    print(f\"   Shape: {X_np.shape}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    print(f\"   Samples: {X.shape[0]:,}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    pos_ratio = y.mean()\n",
    "    scale_pos_weight = (1 - pos_ratio) / pos_ratio\n",
    "    print(f\"\\nğŸ“Š Class distribution:\")\n",
    "    print(f\"   Positive ratio: {pos_ratio:.4f}\")\n",
    "    print(f\"   Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    del X, gdf\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # XGBoost parameters\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'gpu_id': 0,\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    print(\"\\nğŸ”„ Starting cross-validation...\")\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_scores = []\n",
    "    cv_ap = []\n",
    "    cv_wll = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_np, y), 1):\n",
    "        print(f\"\\nğŸ“ Fold {fold}/{n_folds}\")\n",
    "        fold_start = time.time()\n",
    "        \n",
    "        # Create DMatrix\n",
    "        print(f\"   Train: {len(train_idx):,} | Val: {len(val_idx):,}\")\n",
    "        dtrain = xgb.DMatrix(X_np[train_idx], label=y[train_idx])\n",
    "        dval = xgb.DMatrix(X_np[val_idx], label=y[val_idx])\n",
    "        \n",
    "        # Train\n",
    "        print(\"   Training...\")\n",
    "        model = xgb.train(\n",
    "            params, dtrain,\n",
    "            num_boost_round=200,\n",
    "            evals=[(dval, 'val')],\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(dval)\n",
    "        score, ap, wll = calculate_competition_score(y[val_idx], y_pred)\n",
    "        \n",
    "        cv_scores.append(score)\n",
    "        cv_ap.append(ap)\n",
    "        cv_wll.append(wll)\n",
    "        \n",
    "        print(f\"   ğŸ“Š Results:\")\n",
    "        print(f\"      Score: {score:.6f}\")\n",
    "        print(f\"      AP: {ap:.6f}\")\n",
    "        print(f\"      WLL: {wll:.6f}\")\n",
    "        print(f\"      Best iteration: {model.best_iteration}\")\n",
    "        print(f\"   â±ï¸ Time: {time.time() - fold_start:.1f}s\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del dtrain, dval, model\n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    # Final results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“Š Final Cross-Validation Results\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nğŸ† Competition Score: {np.mean(cv_scores):.6f} Â± {np.std(cv_scores):.6f}\")\n",
    "    print(f\"ğŸ“ˆ Average Precision: {np.mean(cv_ap):.6f} Â± {np.std(cv_ap):.6f}\")\n",
    "    print(f\"ğŸ“‰ Weighted LogLoss: {np.mean(cv_wll):.6f} Â± {np.std(cv_wll):.6f}\")\n",
    "    \n",
    "    print(f\"\\nAll fold scores: {[f'{s:.6f}' for s in cv_scores]}\")\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Run cross-validation\n",
    "cv_scores = run_cv(processed_dir, N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
      "COMPLETE!\n",
      "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
      "\n",
      "âœ… Final CV Score: 0.353164 Â± 0.000305\n",
      "âœ… Full dataset processed (10.7M rows)\n",
      "âœ… XGBoost-optimized preprocessing (no normalization)\n",
      "âœ… Memory-efficient with small partitions\n",
      "======================================================================\n",
      "ğŸ§¹ GPU memory cleared\n",
      "\n",
      "ğŸ§¹ Final cleanup complete\n",
      "ğŸ’¾ CPU: 30.8GB/503.5GB (6.9%)\n",
      "ğŸ’¾ GPU: 0.9GB/45.0GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final summary\n",
    "if cv_scores:\n",
    "    print(\"\\n\" + \"ğŸ‰\"*35)\n",
    "    print(\"COMPLETE!\")\n",
    "    print(\"ğŸ‰\"*35)\n",
    "    print(f\"\\nâœ… Final CV Score: {np.mean(cv_scores):.6f} Â± {np.std(cv_scores):.6f}\")\n",
    "    print(\"âœ… Full dataset processed (10.7M rows)\")\n",
    "    print(\"âœ… XGBoost-optimized preprocessing (no normalization)\")\n",
    "    print(\"âœ… Memory-efficient with small partitions\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Cross-validation did not complete. Please check for errors above.\")\n",
    "\n",
    "# Final cleanup\n",
    "clear_gpu_memory()\n",
    "print(\"\\nğŸ§¹ Final cleanup complete\")\n",
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Submission Configuration:\n",
      "   Test data: data/test.parquet\n",
      "   Workflow: data/nvt_processed_final/workflow\n",
      "   Submission file: data/submission.csv\n",
      "   Temp test dir: tmp/nvt_processed_test\n"
     ]
    }
   ],
   "source": [
    "# Configuration for submission\n",
    "OUTPUT_DIR = 'data/nvt_processed_final' # Define the output directory path\n",
    "TEST_PATH = 'data/test.parquet'\n",
    "WORKFLOW_PATH = f'{OUTPUT_DIR}/workflow'\n",
    "SUBMISSION_PATH = 'data/submission.csv'\n",
    "TEMP_TEST_DIR = 'tmp/nvt_processed_test' # Processed test data temp dir\n",
    "\n",
    "print(\"ğŸ“‹ Submission Configuration:\")\n",
    "print(f\"   Test data: {TEST_PATH}\")\n",
    "print(f\"   Workflow: {WORKFLOW_PATH}\")\n",
    "print(f\"   Submission file: {SUBMISSION_PATH}\")\n",
    "print(f\"   Temp test dir: {TEMP_TEST_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸš€ Training Final Model on Full Data\n",
      "======================================================================\n",
      "\n",
      "ğŸ“¦ Loading processed training data...\n",
      "   âœ… Loaded 10,704,179 rows in 5.4s\n",
      "ğŸ’¾ CPU: 63.0GB/503.5GB (17.5%)\n",
      "ğŸ’¾ GPU: 6.1GB/45.0GB\n",
      "\n",
      "ğŸ“Š Preparing DMatrix...\n",
      "   âœ… Full training DMatrix created.\n",
      "\n",
      "ğŸ’ª Training for 200 rounds...\n",
      "[0]\ttrain-logloss:0.67900\n",
      "[50]\ttrain-logloss:0.58576\n",
      "[100]\ttrain-logloss:0.57417\n",
      "[150]\ttrain-logloss:0.56572\n",
      "[199]\ttrain-logloss:0.55912\n",
      "   âœ… Model trained in 20.2s\n"
     ]
    }
   ],
   "source": [
    "def train_final_model(processed_dir, params, num_rounds=200):\n",
    "    \"\"\"\n",
    "    Train the final XGBoost model on the entire processed training dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸš€ Training Final Model on Full Data\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. Load full training data\n",
    "    print(\"\\nğŸ“¦ Loading processed training data...\")\n",
    "    start_load = time.time()\n",
    "    dataset = Dataset(processed_dir, engine='parquet', part_size='256MB')\n",
    "    gdf = dataset.to_ddf().compute()\n",
    "    print(f\"   âœ… Loaded {len(gdf):,} rows in {time.time() - start_load:.1f}s\")\n",
    "    print_memory()\n",
    "\n",
    "    # 2. Prepare data\n",
    "    print(\"\\nğŸ“Š Preparing DMatrix...\")\n",
    "    y = gdf['clicked']\n",
    "    X = gdf.drop('clicked', axis=1)\n",
    "    \n",
    "    # Ensure float32 for XGBoost\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype != 'float32':\n",
    "            X[col] = X[col].astype('float32')\n",
    "            \n",
    "    dtrain_full = xgb.DMatrix(X, label=y)\n",
    "    print(\"   âœ… Full training DMatrix created.\")\n",
    "\n",
    "    del gdf, X, y\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # 3. Train model\n",
    "    print(f\"\\nğŸ’ª Training for {num_rounds} rounds...\")\n",
    "    start_train = time.time()\n",
    "    model = xgb.train(params, dtrain_full, num_boost_round=num_rounds, evals=[(dtrain_full, 'train')], verbose_eval=50)\n",
    "    print(f\"   âœ… Model trained in {time.time() - start_train:.1f}s\")\n",
    "    \n",
    "    del dtrain_full\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Use the same parameters from CV\n",
    "pos_ratio = 0.0191 # From CV output, to recalculate scale_pos_weight\n",
    "scale_pos_weight = (1 - pos_ratio) / pos_ratio\n",
    "\n",
    "final_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'gpu_id': 0,\n",
    "    'verbosity': 1, # Set to 1 to see training progress\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# The CV showed that 200 rounds is optimal\n",
    "final_model = train_final_model(processed_dir, final_params, num_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“„ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘ (ëª¨ë“  ë¬¸ì œ í•´ê²° ìµœì¢… ë²„ì „)\n",
      "======================================================================\n",
      "\n",
      "ğŸ” data/nvt_processed_final/workflowì—ì„œ ì›Œí¬í”Œë¡œìš° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...\n",
      "   âœ… ì›Œí¬í”Œë¡œìš° ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ.\n",
      "\n",
      "ğŸ”§ CPUì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì •ë ¬ ì¤‘...\n",
      "   âœ… ì •ë ¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.\n",
      "\n",
      "ğŸ’¾ ì²˜ë¦¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜ ë° ì €ì¥ ì¤‘...\n",
      "   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ.\n",
      "\n",
      "ğŸ“¦ ì˜ˆì¸¡ì„ ìœ„í•´ ì²˜ë¦¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...\n",
      "   âœ… 1,527,298ê°œ í…ŒìŠ¤íŠ¸ í–‰ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ.\n",
      "\n",
      "ğŸ§  í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ì¤‘...\n",
      "   âœ… ì˜ˆì¸¡ ì™„ë£Œ.\n",
      "\n",
      "âœï¸ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\n",
      "   âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: data/submission.csv\n",
      "   ë¯¸ë¦¬ë³´ê¸°:\n",
      "             ID   clicked\n",
      "0  TEST_0000000  0.723914\n",
      "1  TEST_0000001  0.655095\n",
      "2  TEST_0000002  0.728249\n",
      "3  TEST_0000003  0.407347\n",
      "4  TEST_0000004  0.832982\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "import dask.dataframe as dd\n",
    "import xgboost as xgb\n",
    "import nvtabular as nvt\n",
    "from merlin.io import Dataset\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import cupy as cp\n",
    "\n",
    "# ë…¸íŠ¸ë¶ì˜ í—¬í¼ í•¨ìˆ˜\n",
    "def clear_gpu_memory():\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    gc.collect()\n",
    "\n",
    "def create_submission(model, workflow_path, test_path, submission_path, temp_dir):\n",
    "    \"\"\"\n",
    "    í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³ , ì˜ˆì¸¡í•˜ë©°, ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“„ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘ (ëª¨ë“  ë¬¸ì œ í•´ê²° ìµœì¢… ë²„ì „)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. ì›ë³¸ ì›Œí¬í”Œë¡œìš° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    print(f\"\\nğŸ” {workflow_path}ì—ì„œ ì›Œí¬í”Œë¡œìš° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...\")\n",
    "    workflow = nvt.Workflow.load(workflow_path)\n",
    "    print(\"   âœ… ì›Œí¬í”Œë¡œìš° ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ.\")\n",
    "\n",
    "    # 2. [ì˜¤ë¥˜ ìˆ˜ì • 1] CPU(Pandas)ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì½ê³  ID('seq') ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n",
    "    print(\"\\nğŸ”§ CPUì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì •ë ¬ ì¤‘...\")\n",
    "    test_df = pd.read_parquet(test_path)\n",
    "    test_df = test_df.sort_values('seq').reset_index(drop=True)\n",
    "    \n",
    "    # ì›Œí¬í”Œë¡œìš° í†µê³¼ë¥¼ ìœ„í•œ ë”ë¯¸ 'clicked' ì»¬ëŸ¼ ì¶”ê°€\n",
    "    test_df['clicked'] = 0\n",
    "    test_df['clicked'] = test_df['clicked'].astype('int8')\n",
    "    \n",
    "    # ì •ë ¬ëœ Pandas DataFrameì„ Dataset ê°ì²´ë¡œ ë§Œë“¤ì–´ íƒ€ì… ë¬¸ì œë¥¼ í•´ê²°\n",
    "    test_dataset_sorted = Dataset(test_df, cpu=True)\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    print(\"   âœ… ì •ë ¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.\")\n",
    "\n",
    "    # 3. ì •ë ¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ (ìˆœì„œ ë³´ì¡´)\n",
    "    print(\"\\nğŸ’¾ ì²˜ë¦¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜ ë° ì €ì¥ ì¤‘...\")\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    workflow.transform(test_dataset_sorted).to_parquet(output_path=temp_dir, shuffle=False)\n",
    "    print(\"   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ.\")\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # 4. [ì˜¤ë¥˜ ìˆ˜ì • 2] ì•ˆì •ì ì¸ Dask/Pandasë¡œ ì²˜ë¦¬ëœ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    print(\"\\nğŸ“¦ ì˜ˆì¸¡ì„ ìœ„í•´ ì²˜ë¦¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...\")\n",
    "    processed_pandas_df = dd.read_parquet(temp_dir).compute()\n",
    "    processed_test_gdf = cudf.DataFrame.from_pandas(processed_pandas_df)\n",
    "    del processed_pandas_df\n",
    "    gc.collect()\n",
    "    print(f\"   âœ… {len(processed_test_gdf):,}ê°œ í…ŒìŠ¤íŠ¸ í–‰ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ.\")\n",
    "\n",
    "    # ë”ë¯¸ 'clicked' ì»¬ëŸ¼ ì‚­ì œ\n",
    "    feature_cols = [col for col in processed_test_gdf.columns if col != 'clicked']\n",
    "    processed_test_gdf = processed_test_gdf[feature_cols]\n",
    "\n",
    "    # ë°ì´í„° íƒ€ì… ë³€í™˜ ë° DMatrix ìƒì„±\n",
    "    for col in processed_test_gdf.columns:\n",
    "        if processed_test_gdf[col].dtype != 'float32':\n",
    "            processed_test_gdf[col] = processed_test_gdf[col].astype('float32')\n",
    "    dtest = xgb.DMatrix(processed_test_gdf)\n",
    "    del processed_test_gdf\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # 5. ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "    print(\"\\nğŸ§  í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ì¤‘...\")\n",
    "    predictions = model.predict(dtest)\n",
    "    print(\"   âœ… ì˜ˆì¸¡ ì™„ë£Œ.\")\n",
    "    del dtest\n",
    "\n",
    "    # 6. ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    print(\"\\nâœï¸ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "    SAMPLE_SUBMISSION_PATH = 'sample_submission.csv' \n",
    "    sub_df = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
    "    sub_df = sub_df.sort_values('ID').reset_index(drop=True)\n",
    "    sub_df['clicked'] = predictions\n",
    "    sub_df.to_csv(submission_path, index=False)\n",
    "    print(f\"   âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {submission_path}\")\n",
    "    print(f\"   ë¯¸ë¦¬ë³´ê¸°:\\n{sub_df.head()}\")\n",
    "\n",
    "# í•¨ìˆ˜ ì‹¤í–‰\n",
    "try:\n",
    "    create_submission(final_model, WORKFLOW_PATH, TEST_PATH, SUBMISSION_PATH, TEMP_TEST_DIR)\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
