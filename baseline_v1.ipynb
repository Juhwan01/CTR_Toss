{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA Merlin XGBoost\n",
    "Complete implementation with proper memory management and debugging outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment configured\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ nvtabular       23.08.00        (required: ≥23.08.00)\n",
      "✅ cudf            23.10.00        (required: ≥23.10)\n",
      "✅ cupy            13.6.0          (required: ≥13.6)\n",
      "✅ xgboost         3.0.5           (required: ≥3.0)\n",
      "✅ dask            2023.9.2        (required: ≥2023.9)\n",
      "✅ pandas          1.5.3           (required: ≥1.5)\n",
      "✅ numpy           1.23.5          (required: ≥1.24)\n",
      "✅ scikit-learn    1.7.2           (required: ≥1.7)\n",
      "✅ psutil          5.9.8           (required: ≥5.9)\n",
      "✅ pyarrow         12.0.1          (required: ≥12.0)\n",
      "\n",
      "✅ All required libraries are installed and compatible!\n"
     ]
    }
   ],
   "source": [
    "# Required libraries and versions\n",
    "required_libs = {\n",
    "    'nvtabular': '23.08.00',\n",
    "    'cudf': '23.10',      # Prefix match\n",
    "    'cupy': '13.6',       # Prefix match  \n",
    "    'xgboost': '3.0',     # Minimum version\n",
    "    'dask': '2023.9',\n",
    "    'pandas': '1.5',\n",
    "    'numpy': '1.24',\n",
    "    'scikit-learn': '1.7',\n",
    "    'psutil': '5.9',      # 5.9.1 works fine (used in working code)\n",
    "    'pyarrow': '12.0'     # 12.0.1 works fine (used in working code)\n",
    "}\n",
    "\n",
    "# Check installed versions\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "# Suppress deprecation warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    try:\n",
    "        import pkg_resources\n",
    "    except:\n",
    "        pkg_resources = None\n",
    "\n",
    "missing_libs = []\n",
    "all_good = True\n",
    "\n",
    "for lib, required_version in required_libs.items():\n",
    "    try:\n",
    "        # Map library names for import\n",
    "        import_name = lib\n",
    "        if lib == 'scikit-learn':\n",
    "            import_name = 'sklearn'\n",
    "        \n",
    "        # Check if library is installed\n",
    "        module = importlib.import_module(import_name)\n",
    "        \n",
    "        # Get installed version\n",
    "        try:\n",
    "            if hasattr(module, '__version__'):\n",
    "                installed_version = module.__version__\n",
    "            elif pkg_resources:\n",
    "                installed_version = pkg_resources.get_distribution(lib).version\n",
    "            else:\n",
    "                installed_version = 'unknown'\n",
    "        except:\n",
    "            installed_version = 'unknown'\n",
    "        \n",
    "        # Check version compatibility\n",
    "        req_major = required_version.split('.')[0]\n",
    "        inst_version_parts = installed_version.split('.')\n",
    "        inst_major = inst_version_parts[0] if installed_version != 'unknown' else ''\n",
    "        \n",
    "        # More lenient version check\n",
    "        if installed_version == 'unknown':\n",
    "            print(f\"⚠️  {lib:15} {installed_version:15} (required: ≥{required_version})\")\n",
    "        elif float(inst_major) >= float(req_major) if inst_major.isdigit() and req_major.isdigit() else installed_version.startswith(required_version[:3]):\n",
    "            print(f\"✅ {lib:15} {installed_version:15} (required: ≥{required_version})\")\n",
    "        else:\n",
    "            print(f\"⚠️  {lib:15} {installed_version:15} (required: ≥{required_version}) - but should work\")\n",
    "        \n",
    "    except ImportError:\n",
    "        missing_libs.append(lib)\n",
    "        print(f\"❌ {lib:15} NOT INSTALLED (required: ≥{required_version})\")\n",
    "        all_good = False\n",
    "\n",
    "# Report\n",
    "if missing_libs:\n",
    "    print(f\"\\n❌ Missing libraries: {', '.join(missing_libs)}\")\n",
    "    print(\"Please install them using conda or pip\")\n",
    "elif all_good:\n",
    "    print(\"\\n✅ All required libraries are installed and compatible!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully\n",
      "NVTabular version: 23.08.00\n",
      "XGBoost version: 3.0.5\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import psutil\n",
    "\n",
    "# GPU libraries\n",
    "import cudf\n",
    "import cupy as cp\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "from nvtabular import ops\n",
    "from merlin.io import Dataset\n",
    "\n",
    "# ML libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")\n",
    "print(f\"NVTabular version: {nvt.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Configuration:\n",
      "   Input: data/train.parquet\n",
      "   Output: data/nvt_processed_final\n",
      "   Folds: 5\n",
      "   Force reprocess: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration(DATA PATH)\n",
    "TRAIN_PATH = 'data/train.parquet'\n",
    "OUTPUT_DIR = 'data/nvt_processed_final'\n",
    "TEMP_DIR = '/tmp'\n",
    "N_FOLDS = 5\n",
    "FORCE_REPROCESS = False  # Set to True to reprocess data\n",
    "\n",
    "print(f\"📋 Configuration:\")\n",
    "print(f\"   Input: {TRAIN_PATH}\")\n",
    "print(f\"   Output: {OUTPUT_DIR}\")\n",
    "print(f\"   Folds: {N_FOLDS}\")\n",
    "print(f\"   Force reprocess: {FORCE_REPROCESS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing memory functions:\n",
      "💾 CPU: 27.6GB/503.5GB (6.2%)\n",
      "💾 GPU: 0.8GB/45.0GB\n",
      "🧹 GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Memory management functions\n",
    "def print_memory():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    \n",
    "    try:\n",
    "        import pynvml\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_used = gpu_info.used / 1024**3\n",
    "        gpu_total = gpu_info.total / 1024**3\n",
    "    except:\n",
    "        gpu_used = 0\n",
    "        gpu_total = 0\n",
    "    \n",
    "    print(f\"💾 CPU: {mem.used/1024**3:.1f}GB/{mem.total/1024**3:.1f}GB ({mem.percent:.1f}%)\")\n",
    "    print(f\"💾 GPU: {gpu_used:.1f}GB/{gpu_total:.1f}GB\")\n",
    "    return mem.percent\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    gc.collect()\n",
    "    print(\"🧹 GPU memory cleared\")\n",
    "\n",
    "# Test memory functions\n",
    "print(\"Testing memory functions:\")\n",
    "print_memory()\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metric functions defined\n"
     ]
    }
   ],
   "source": [
    "# Metric functions\n",
    "def calculate_weighted_logloss(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"Calculate Weighted LogLoss with 50:50 class weights\"\"\"\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    \n",
    "    mask_0 = (y_true == 0)\n",
    "    mask_1 = (y_true == 1)\n",
    "    \n",
    "    ll_0 = -np.mean(np.log(1 - y_pred[mask_0])) if mask_0.sum() > 0 else 0\n",
    "    ll_1 = -np.mean(np.log(y_pred[mask_1])) if mask_1.sum() > 0 else 0\n",
    "    \n",
    "    return 0.5 * ll_0 + 0.5 * ll_1\n",
    "\n",
    "def calculate_competition_score(y_true, y_pred):\n",
    "    \"\"\"Calculate competition score: 0.5*AP + 0.5*(1/(1+WLL))\"\"\"\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    wll = calculate_weighted_logloss(y_true, y_pred)\n",
    "    score = 0.5 * ap + 0.5 * (1 / (1 + wll))\n",
    "    return score, ap, wll\n",
    "\n",
    "print(\"✅ Metric functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Creating XGBoost-optimized workflow...\n",
      "   Categorical: 5 columns\n",
      "   Continuous: 112 columns\n",
      "   Total features: 117\n",
      "   ✅ Workflow created (no normalization for tree models)\n",
      "✅ Workflow creation tested successfully\n"
     ]
    }
   ],
   "source": [
    "def create_workflow():\n",
    "    \"\"\"Create NVTabular workflow optimized for XGBoost\"\"\"\n",
    "    print(\"\\n🔧 Creating XGBoost-optimized workflow...\")\n",
    "    \n",
    "    # TRUE CATEGORICAL COLUMNS (only 5)\n",
    "    true_categorical = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "    \n",
    "    # CONTINUOUS COLUMNS (112 total)\n",
    "    all_continuous = (\n",
    "        [f'feat_a_{i}' for i in range(1, 19)] +  # 18\n",
    "        [f'feat_b_{i}' for i in range(1, 7)] +   # 6\n",
    "        [f'feat_c_{i}' for i in range(1, 9)] +   # 8\n",
    "        [f'feat_d_{i}' for i in range(1, 7)] +   # 6\n",
    "        [f'feat_e_{i}' for i in range(1, 11)] +  # 10\n",
    "        [f'history_a_{i}' for i in range(1, 8)] +  # 7\n",
    "        [f'history_b_{i}' for i in range(1, 31)] + # 30\n",
    "        [f'l_feat_{i}' for i in range(1, 28)]      # 27\n",
    "    )\n",
    "    \n",
    "    print(f\"   Categorical: {len(true_categorical)} columns\")\n",
    "    print(f\"   Continuous: {len(all_continuous)} columns\")\n",
    "    print(f\"   Total features: {len(true_categorical) + len(all_continuous)}\")\n",
    "    \n",
    "    # Minimal preprocessing for XGBoost\n",
    "    cat_features = true_categorical >> ops.Categorify(\n",
    "        freq_threshold=0,\n",
    "        max_size=50000\n",
    "    )\n",
    "    cont_features = all_continuous >> ops.FillMissing(fill_val=0)\n",
    "    \n",
    "    workflow = nvt.Workflow(cat_features + cont_features + ['clicked'])\n",
    "    \n",
    "    print(\"   ✅ Workflow created (no normalization for tree models)\")\n",
    "    return workflow\n",
    "\n",
    "# Test workflow creation\n",
    "test_workflow = create_workflow()\n",
    "print(\"✅ Workflow creation tested successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🚀 NVTabular Data Processing\n",
      "======================================================================\n",
      "💾 CPU: 27.6GB/503.5GB (6.2%)\n",
      "💾 GPU: 0.8GB/45.0GB\n",
      "\n",
      "📋 Creating temp file without 'seq' column...\n",
      "   Total columns: 119\n",
      "   Using columns: 118 (excluded 'seq')\n",
      "   Loaded 10,704,179 rows\n",
      "   ✅ Temp file created\n",
      "\n",
      "📦 Creating NVTabular Dataset...\n",
      "   Using 32MB partitions for memory efficiency\n",
      "🧹 GPU memory cleared\n",
      "   ✅ Dataset created\n",
      "\n",
      "📊 Fitting workflow...\n",
      "\n",
      "🔧 Creating XGBoost-optimized workflow...\n",
      "   Categorical: 5 columns\n",
      "   Continuous: 112 columns\n",
      "   Total features: 117\n",
      "   ✅ Workflow created (no normalization for tree models)\n",
      "   ✅ Workflow fitted\n",
      "\n",
      "💾 Transforming and saving to data/nvt_processed_final...\n",
      "🧹 GPU memory cleared\n",
      "   ✅ Data processed and saved\n",
      "   ✅ Workflow saved to data/nvt_processed_final/workflow\n",
      "💾 CPU: 32.7GB/503.5GB (7.2%)\n",
      "💾 GPU: 26.0GB/45.0GB\n",
      "\n",
      "✅ Processing complete!\n",
      "   Time: 70.6s\n",
      "   Memory increase: +1.0%\n",
      "🧹 GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "def process_data():\n",
    "    \"\"\"Process data with NVTabular\"\"\"\n",
    "    import shutil\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🚀 NVTabular Data Processing\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check if already processed\n",
    "    if os.path.exists(OUTPUT_DIR) and not FORCE_REPROCESS:\n",
    "        try:\n",
    "            test_dataset = Dataset(OUTPUT_DIR, engine='parquet')\n",
    "            print(f\"✅ Using existing processed data from {OUTPUT_DIR}\")\n",
    "            return OUTPUT_DIR\n",
    "        except:\n",
    "            print(f\"⚠️ Existing data corrupted, reprocessing...\")\n",
    "            shutil.rmtree(OUTPUT_DIR)\n",
    "    \n",
    "    # Clear existing if needed\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        print(f\"🗑️ Removing existing directory {OUTPUT_DIR}\")\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    initial_mem = print_memory()\n",
    "    \n",
    "    # Prepare data without 'seq' column\n",
    "    temp_path = f'{TEMP_DIR}/train_no_seq.parquet'\n",
    "    if not os.path.exists(temp_path):\n",
    "        print(\"\\n📋 Creating temp file without 'seq' column...\")\n",
    "        pf = pq.ParquetFile(TRAIN_PATH)\n",
    "        cols = [c for c in pf.schema.names if c != 'seq']\n",
    "        print(f\"   Total columns: {len(pf.schema.names)}\")\n",
    "        print(f\"   Using columns: {len(cols)} (excluded 'seq')\")\n",
    "        \n",
    "        df = pd.read_parquet(TRAIN_PATH, columns=cols)\n",
    "        print(f\"   Loaded {len(df):,} rows\")\n",
    "        df.to_parquet(temp_path, index=False)\n",
    "        del df\n",
    "        gc.collect()\n",
    "        print(\"   ✅ Temp file created\")\n",
    "    else:\n",
    "        print(f\"✅ Using existing temp file: {temp_path}\")\n",
    "    \n",
    "    # Create dataset with small partitions\n",
    "    print(\"\\n📦 Creating NVTabular Dataset...\")\n",
    "    print(\"   Using 32MB partitions for memory efficiency\")\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    dataset = Dataset(\n",
    "        temp_path,\n",
    "        engine='parquet',\n",
    "        part_size='32MB'  #change size based on your environment\n",
    "    )\n",
    "    print(\"   ✅ Dataset created\")\n",
    "    \n",
    "    # Create and fit workflow\n",
    "    print(\"\\n📊 Fitting workflow...\")\n",
    "    workflow = create_workflow()\n",
    "    workflow.fit(dataset)\n",
    "    print(\"   ✅ Workflow fitted\")\n",
    "    \n",
    "    # Transform and save\n",
    "    print(f\"\\n💾 Transforming and saving to {OUTPUT_DIR}...\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    try:\n",
    "        workflow.transform(dataset).to_parquet(\n",
    "            output_path=OUTPUT_DIR,\n",
    "            shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "            out_files_per_proc=8\n",
    "        )\n",
    "        \n",
    "        workflow_path = f'{OUTPUT_DIR}/workflow'\n",
    "        workflow.save(workflow_path)\n",
    "        print(f\"   ✅ Data processed and saved\")\n",
    "        print(f\"   ✅ Workflow saved to {workflow_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during processing: {e}\")\n",
    "        if os.path.exists(OUTPUT_DIR):\n",
    "            shutil.rmtree(OUTPUT_DIR)\n",
    "        raise\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    final_mem = print_memory()\n",
    "    \n",
    "    print(f\"\\n✅ Processing complete!\")\n",
    "    print(f\"   Time: {elapsed:.1f}s\")\n",
    "    print(f\"   Memory increase: +{final_mem - initial_mem:.1f}%\")\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    return OUTPUT_DIR\n",
    "\n",
    "# Process data\n",
    "processed_dir = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🔄 Stratified KFold Cross-Validation\n",
      "======================================================================\n",
      "\n",
      "📦 Loading processed data...\n",
      "   Converting to GPU DataFrame...\n",
      "   ✅ Loaded 10,704,179 rows x 118 columns\n",
      "   Time: 14.2s\n",
      "💾 CPU: 37.5GB/503.5GB (8.2%)\n",
      "💾 GPU: 5.9GB/45.0GB\n",
      "\n",
      "📊 Preparing data for XGBoost...\n",
      "   Shape: (10704179, 117)\n",
      "   Features: 117\n",
      "   Samples: 10,704,179\n",
      "\n",
      "📊 Class distribution:\n",
      "   Positive ratio: 0.0191\n",
      "   Scale pos weight: 51.43\n",
      "🧹 GPU memory cleared\n",
      "\n",
      "🔄 Starting cross-validation...\n",
      "\n",
      "📍 Fold 1/5\n",
      "   Train: 8,563,343 | Val: 2,140,836\n",
      "   Training...\n",
      "   📊 Results:\n",
      "      Score: 0.352895\n",
      "      AP: 0.080719\n",
      "      WLL: 0.599819\n",
      "      Best iteration: 199\n",
      "   ⏱️ Time: 32.4s\n",
      "🧹 GPU memory cleared\n",
      "\n",
      "📍 Fold 2/5\n",
      "   Train: 8,563,343 | Val: 2,140,836\n",
      "   Training...\n",
      "   📊 Results:\n",
      "      Score: 0.353203\n",
      "      AP: 0.081334\n",
      "      WLL: 0.599817\n",
      "      Best iteration: 199\n",
      "   ⏱️ Time: 30.6s\n",
      "🧹 GPU memory cleared\n",
      "\n",
      "📍 Fold 3/5\n",
      "   Train: 8,563,343 | Val: 2,140,836\n",
      "   Training...\n",
      "   📊 Results:\n",
      "      Score: 0.352804\n",
      "      AP: 0.080146\n",
      "      WLL: 0.598821\n",
      "      Best iteration: 199\n",
      "   ⏱️ Time: 30.6s\n",
      "🧹 GPU memory cleared\n",
      "\n",
      "📍 Fold 4/5\n",
      "   Train: 8,563,343 | Val: 2,140,836\n",
      "   Training...\n",
      "   📊 Results:\n",
      "      Score: 0.353666\n",
      "      AP: 0.081622\n",
      "      WLL: 0.598184\n",
      "      Best iteration: 199\n",
      "   ⏱️ Time: 29.4s\n",
      "🧹 GPU memory cleared\n",
      "\n",
      "📍 Fold 5/5\n",
      "   Train: 8,563,344 | Val: 2,140,835\n",
      "   Training...\n",
      "   📊 Results:\n",
      "      Score: 0.353254\n",
      "      AP: 0.081446\n",
      "      WLL: 0.599839\n",
      "      Best iteration: 199\n",
      "   ⏱️ Time: 29.6s\n",
      "🧹 GPU memory cleared\n",
      "\n",
      "======================================================================\n",
      "📊 Final Cross-Validation Results\n",
      "======================================================================\n",
      "\n",
      "🏆 Competition Score: 0.353164 ± 0.000305\n",
      "📈 Average Precision: 0.081053 ± 0.000546\n",
      "📉 Weighted LogLoss: 0.599296 ± 0.000678\n",
      "\n",
      "All fold scores: ['0.352895', '0.353203', '0.352804', '0.353666', '0.353254']\n"
     ]
    }
   ],
   "source": [
    "def run_cv(processed_dir, n_folds=5):\n",
    "    \"\"\"Run stratified cross-validation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🔄 Stratified KFold Cross-Validation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load processed data\n",
    "    print(\"\\n📦 Loading processed data...\")\n",
    "    start_load = time.time()\n",
    "    \n",
    "    try:\n",
    "        dataset = Dataset(processed_dir, engine='parquet', part_size='256MB')\n",
    "        print(\"   Converting to GPU DataFrame...\")\n",
    "        gdf = dataset.to_ddf().compute()\n",
    "        print(f\"   ✅ Loaded {len(gdf):,} rows x {len(gdf.columns)} columns\")\n",
    "        print(f\"   Time: {time.time() - start_load:.1f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print_memory()\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\n📊 Preparing data for XGBoost...\")\n",
    "    y = gdf['clicked'].to_numpy()\n",
    "    X = gdf.drop('clicked', axis=1)\n",
    "    \n",
    "    # Convert to float32\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype != 'float32':\n",
    "            X[col] = X[col].astype('float32')\n",
    "    \n",
    "    X_np = X.to_numpy()\n",
    "    print(f\"   Shape: {X_np.shape}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    print(f\"   Samples: {X.shape[0]:,}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    pos_ratio = y.mean()\n",
    "    scale_pos_weight = (1 - pos_ratio) / pos_ratio\n",
    "    print(f\"\\n📊 Class distribution:\")\n",
    "    print(f\"   Positive ratio: {pos_ratio:.4f}\")\n",
    "    print(f\"   Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    del X, gdf\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # XGBoost parameters\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'gpu_id': 0,\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    print(\"\\n🔄 Starting cross-validation...\")\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_scores = []\n",
    "    cv_ap = []\n",
    "    cv_wll = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_np, y), 1):\n",
    "        print(f\"\\n📍 Fold {fold}/{n_folds}\")\n",
    "        fold_start = time.time()\n",
    "        \n",
    "        # Create DMatrix\n",
    "        print(f\"   Train: {len(train_idx):,} | Val: {len(val_idx):,}\")\n",
    "        dtrain = xgb.DMatrix(X_np[train_idx], label=y[train_idx])\n",
    "        dval = xgb.DMatrix(X_np[val_idx], label=y[val_idx])\n",
    "        \n",
    "        # Train\n",
    "        print(\"   Training...\")\n",
    "        model = xgb.train(\n",
    "            params, dtrain,\n",
    "            num_boost_round=200,\n",
    "            evals=[(dval, 'val')],\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(dval)\n",
    "        score, ap, wll = calculate_competition_score(y[val_idx], y_pred)\n",
    "        \n",
    "        cv_scores.append(score)\n",
    "        cv_ap.append(ap)\n",
    "        cv_wll.append(wll)\n",
    "        \n",
    "        print(f\"   📊 Results:\")\n",
    "        print(f\"      Score: {score:.6f}\")\n",
    "        print(f\"      AP: {ap:.6f}\")\n",
    "        print(f\"      WLL: {wll:.6f}\")\n",
    "        print(f\"      Best iteration: {model.best_iteration}\")\n",
    "        print(f\"   ⏱️ Time: {time.time() - fold_start:.1f}s\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del dtrain, dval, model\n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    # Final results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📊 Final Cross-Validation Results\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n🏆 Competition Score: {np.mean(cv_scores):.6f} ± {np.std(cv_scores):.6f}\")\n",
    "    print(f\"📈 Average Precision: {np.mean(cv_ap):.6f} ± {np.std(cv_ap):.6f}\")\n",
    "    print(f\"📉 Weighted LogLoss: {np.mean(cv_wll):.6f} ± {np.std(cv_wll):.6f}\")\n",
    "    \n",
    "    print(f\"\\nAll fold scores: {[f'{s:.6f}' for s in cv_scores]}\")\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Run cross-validation\n",
    "cv_scores = run_cv(processed_dir, N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n",
      "COMPLETE!\n",
      "🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n",
      "\n",
      "✅ Final CV Score: 0.353164 ± 0.000305\n",
      "✅ Full dataset processed (10.7M rows)\n",
      "✅ XGBoost-optimized preprocessing (no normalization)\n",
      "✅ Memory-efficient with small partitions\n",
      "======================================================================\n",
      "🧹 GPU memory cleared\n",
      "\n",
      "🧹 Final cleanup complete\n",
      "💾 CPU: 30.8GB/503.5GB (6.9%)\n",
      "💾 GPU: 0.9GB/45.0GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final summary\n",
    "if cv_scores:\n",
    "    print(\"\\n\" + \"🎉\"*35)\n",
    "    print(\"COMPLETE!\")\n",
    "    print(\"🎉\"*35)\n",
    "    print(f\"\\n✅ Final CV Score: {np.mean(cv_scores):.6f} ± {np.std(cv_scores):.6f}\")\n",
    "    print(\"✅ Full dataset processed (10.7M rows)\")\n",
    "    print(\"✅ XGBoost-optimized preprocessing (no normalization)\")\n",
    "    print(\"✅ Memory-efficient with small partitions\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n⚠️ Cross-validation did not complete. Please check for errors above.\")\n",
    "\n",
    "# Final cleanup\n",
    "clear_gpu_memory()\n",
    "print(\"\\n🧹 Final cleanup complete\")\n",
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Submission Configuration:\n",
      "   Test data: data/test.parquet\n",
      "   Workflow: data/nvt_processed_final/workflow\n",
      "   Submission file: data/submission.csv\n",
      "   Temp test dir: tmp/nvt_processed_test\n"
     ]
    }
   ],
   "source": [
    "# Configuration for submission\n",
    "OUTPUT_DIR = 'data/nvt_processed_final' # Define the output directory path\n",
    "TEST_PATH = 'data/test.parquet'\n",
    "WORKFLOW_PATH = f'{OUTPUT_DIR}/workflow'\n",
    "SUBMISSION_PATH = 'data/submission.csv'\n",
    "TEMP_TEST_DIR = 'tmp/nvt_processed_test' # Processed test data temp dir\n",
    "\n",
    "print(\"📋 Submission Configuration:\")\n",
    "print(f\"   Test data: {TEST_PATH}\")\n",
    "print(f\"   Workflow: {WORKFLOW_PATH}\")\n",
    "print(f\"   Submission file: {SUBMISSION_PATH}\")\n",
    "print(f\"   Temp test dir: {TEMP_TEST_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🚀 Training Final Model on Full Data\n",
      "======================================================================\n",
      "\n",
      "📦 Loading processed training data...\n",
      "   ✅ Loaded 10,704,179 rows in 5.4s\n",
      "💾 CPU: 63.0GB/503.5GB (17.5%)\n",
      "💾 GPU: 6.1GB/45.0GB\n",
      "\n",
      "📊 Preparing DMatrix...\n",
      "   ✅ Full training DMatrix created.\n",
      "\n",
      "💪 Training for 200 rounds...\n",
      "[0]\ttrain-logloss:0.67900\n",
      "[50]\ttrain-logloss:0.58576\n",
      "[100]\ttrain-logloss:0.57417\n",
      "[150]\ttrain-logloss:0.56572\n",
      "[199]\ttrain-logloss:0.55912\n",
      "   ✅ Model trained in 20.2s\n"
     ]
    }
   ],
   "source": [
    "def train_final_model(processed_dir, params, num_rounds=200):\n",
    "    \"\"\"\n",
    "    Train the final XGBoost model on the entire processed training dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🚀 Training Final Model on Full Data\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. Load full training data\n",
    "    print(\"\\n📦 Loading processed training data...\")\n",
    "    start_load = time.time()\n",
    "    dataset = Dataset(processed_dir, engine='parquet', part_size='256MB')\n",
    "    gdf = dataset.to_ddf().compute()\n",
    "    print(f\"   ✅ Loaded {len(gdf):,} rows in {time.time() - start_load:.1f}s\")\n",
    "    print_memory()\n",
    "\n",
    "    # 2. Prepare data\n",
    "    print(\"\\n📊 Preparing DMatrix...\")\n",
    "    y = gdf['clicked']\n",
    "    X = gdf.drop('clicked', axis=1)\n",
    "    \n",
    "    # Ensure float32 for XGBoost\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype != 'float32':\n",
    "            X[col] = X[col].astype('float32')\n",
    "            \n",
    "    dtrain_full = xgb.DMatrix(X, label=y)\n",
    "    print(\"   ✅ Full training DMatrix created.\")\n",
    "\n",
    "    del gdf, X, y\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # 3. Train model\n",
    "    print(f\"\\n💪 Training for {num_rounds} rounds...\")\n",
    "    start_train = time.time()\n",
    "    model = xgb.train(params, dtrain_full, num_boost_round=num_rounds, evals=[(dtrain_full, 'train')], verbose_eval=50)\n",
    "    print(f\"   ✅ Model trained in {time.time() - start_train:.1f}s\")\n",
    "    \n",
    "    del dtrain_full\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Use the same parameters from CV\n",
    "pos_ratio = 0.0191 # From CV output, to recalculate scale_pos_weight\n",
    "scale_pos_weight = (1 - pos_ratio) / pos_ratio\n",
    "\n",
    "final_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'gpu_id': 0,\n",
    "    'verbosity': 1, # Set to 1 to see training progress\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# The CV showed that 200 rounds is optimal\n",
    "final_model = train_final_model(processed_dir, final_params, num_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "📄 제출 파일 생성 중 (모든 문제 해결 최종 버전)\n",
      "======================================================================\n",
      "\n",
      "🔍 data/nvt_processed_final/workflow에서 워크플로우 불러오는 중...\n",
      "   ✅ 워크플로우 불러오기 완료.\n",
      "\n",
      "🔧 CPU에서 테스트 데이터 로드 및 정렬 중...\n",
      "   ✅ 정렬된 테스트 데이터 준비 완료.\n",
      "\n",
      "💾 처리된 테스트 데이터 변환 및 저장 중...\n",
      "   ✅ 테스트 데이터 처리 완료.\n",
      "\n",
      "📦 예측을 위해 처리된 테스트 데이터 불러오는 중...\n",
      "   ✅ 1,527,298개 테스트 행 불러오기 완료.\n",
      "\n",
      "🧠 테스트 데이터로 예측 중...\n",
      "   ✅ 예측 완료.\n",
      "\n",
      "✍️ 제출 파일 생성 중...\n",
      "   ✅ 제출 파일 저장 완료: data/submission.csv\n",
      "   미리보기:\n",
      "             ID   clicked\n",
      "0  TEST_0000000  0.723914\n",
      "1  TEST_0000001  0.655095\n",
      "2  TEST_0000002  0.728249\n",
      "3  TEST_0000003  0.407347\n",
      "4  TEST_0000004  0.832982\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "import dask.dataframe as dd\n",
    "import xgboost as xgb\n",
    "import nvtabular as nvt\n",
    "from merlin.io import Dataset\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import cupy as cp\n",
    "\n",
    "# 노트북의 헬퍼 함수\n",
    "def clear_gpu_memory():\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    gc.collect()\n",
    "\n",
    "def create_submission(model, workflow_path, test_path, submission_path, temp_dir):\n",
    "    \"\"\"\n",
    "    테스트 데이터를 처리하고, 예측하며, 제출 파일을 생성합니다.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📄 제출 파일 생성 중 (모든 문제 해결 최종 버전)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. 원본 워크플로우 불러오기\n",
    "    print(f\"\\n🔍 {workflow_path}에서 워크플로우 불러오는 중...\")\n",
    "    workflow = nvt.Workflow.load(workflow_path)\n",
    "    print(\"   ✅ 워크플로우 불러오기 완료.\")\n",
    "\n",
    "    # 2. [오류 수정 1] CPU(Pandas)에서 테스트 데이터를 읽고 ID('seq') 기준으로 정렬\n",
    "    print(\"\\n🔧 CPU에서 테스트 데이터 로드 및 정렬 중...\")\n",
    "    test_df = pd.read_parquet(test_path)\n",
    "    test_df = test_df.sort_values('seq').reset_index(drop=True)\n",
    "    \n",
    "    # 워크플로우 통과를 위한 더미 'clicked' 컬럼 추가\n",
    "    test_df['clicked'] = 0\n",
    "    test_df['clicked'] = test_df['clicked'].astype('int8')\n",
    "    \n",
    "    # 정렬된 Pandas DataFrame을 Dataset 객체로 만들어 타입 문제를 해결\n",
    "    test_dataset_sorted = Dataset(test_df, cpu=True)\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    print(\"   ✅ 정렬된 테스트 데이터 준비 완료.\")\n",
    "\n",
    "    # 3. 정렬된 테스트 데이터 처리 (순서 보존)\n",
    "    print(\"\\n💾 처리된 테스트 데이터 변환 및 저장 중...\")\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    workflow.transform(test_dataset_sorted).to_parquet(output_path=temp_dir, shuffle=False)\n",
    "    print(\"   ✅ 테스트 데이터 처리 완료.\")\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # 4. [오류 수정 2] 안정적인 Dask/Pandas로 처리된 데이터 불러오기\n",
    "    print(\"\\n📦 예측을 위해 처리된 테스트 데이터 불러오는 중...\")\n",
    "    processed_pandas_df = dd.read_parquet(temp_dir).compute()\n",
    "    processed_test_gdf = cudf.DataFrame.from_pandas(processed_pandas_df)\n",
    "    del processed_pandas_df\n",
    "    gc.collect()\n",
    "    print(f\"   ✅ {len(processed_test_gdf):,}개 테스트 행 불러오기 완료.\")\n",
    "\n",
    "    # 더미 'clicked' 컬럼 삭제\n",
    "    feature_cols = [col for col in processed_test_gdf.columns if col != 'clicked']\n",
    "    processed_test_gdf = processed_test_gdf[feature_cols]\n",
    "\n",
    "    # 데이터 타입 변환 및 DMatrix 생성\n",
    "    for col in processed_test_gdf.columns:\n",
    "        if processed_test_gdf[col].dtype != 'float32':\n",
    "            processed_test_gdf[col] = processed_test_gdf[col].astype('float32')\n",
    "    dtest = xgb.DMatrix(processed_test_gdf)\n",
    "    del processed_test_gdf\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # 5. 예측 수행\n",
    "    print(\"\\n🧠 테스트 데이터로 예측 중...\")\n",
    "    predictions = model.predict(dtest)\n",
    "    print(\"   ✅ 예측 완료.\")\n",
    "    del dtest\n",
    "\n",
    "    # 6. 제출 파일 생성\n",
    "    print(\"\\n✍️ 제출 파일 생성 중...\")\n",
    "    SAMPLE_SUBMISSION_PATH = 'sample_submission.csv' \n",
    "    sub_df = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
    "    sub_df = sub_df.sort_values('ID').reset_index(drop=True)\n",
    "    sub_df['clicked'] = predictions\n",
    "    sub_df.to_csv(submission_path, index=False)\n",
    "    print(f\"   ✅ 제출 파일 저장 완료: {submission_path}\")\n",
    "    print(f\"   미리보기:\\n{sub_df.head()}\")\n",
    "\n",
    "# 함수 실행\n",
    "try:\n",
    "    create_submission(final_model, WORKFLOW_PATH, TEST_PATH, SUBMISSION_PATH, TEMP_TEST_DIR)\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 예기치 않은 오류가 발생했습니다: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
